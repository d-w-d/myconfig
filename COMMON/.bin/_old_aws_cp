#!/bin/bash
#
# Wraps aws s3 commands for fast uploads of files/dir with web optimizations, etc.
# NOTE: this is an old version that attempted to allow a lot of flexibility for uploading
#         files/dirs to buckets at arbitrary extensions; it was replaced with a simpler
#         and more restrictive CLI

#######################################
# Print usage
# Globals:
#   None
# Arguments:
#   None
#######################################
function _usage() {
  echo -e """${WHI}
This is a wrapper around \`aws s3\` to quickly upload files/dirs to an S3 bucket.
It assumes you have a bucket prepared with a bucket policy allowing public access
to all objects in the bucket. It does not set ACL permissions.

All html files are first copied, renamed (e.g. index.html -> index) and both
are uploaded into order to faciliate SPA routing.

All media files are first gzip-ed before uploading with headers for efficient serving.
${GRE}
=============================================================================
${BLU}_aws_cp ls   ${WHI}                 ${GRE}List buckets
-----------
${BLU}_aws_cp put  ${WHI}FILE BUCKET      ${GRE}Put file into BUCKET (root folder)
${BLU}_aws_cp put  ${WHI}FILE BUCKET/A    ${GRE}Put file into folder A in BUCKET
${BLU}_aws_cp put  ${WHI}DIR BUCKET       ${GRE}Put contents of DIR into BUCKET (root folder)
${BLU}_aws_cp put  ${WHI}DIR BUCKET/A     ${GRE}Put contents of DIR into folder A in BUCKET
${BLU}_aws_cp put  ${WHI}DIR BUCKET/A/DIR ${GRE}Put DIR into folder A in BUCKET
-----------
${BLU}_aws_cp get  ${WHI}BUCKET/A/DIR     ${GRE}Get: to be implemented
${BLU}_aws_cp sync ${WHI}BUCKET/A/DIR     ${GRE}Get: to be implemented
=============================================================================${WHI}
"""
}

#######################################
# Print available buckets
# Globals:
#   None
# Arguments:
#   None
#######################################
function _ls() {
  aws s3 ls
}

#######################################
# Check that bucket exists for
# these AWS credentials
# Globals:
#   None
# Arguments:
#   $1 = bucket to check if exists
#######################################
function _does_bucket_exist() {
  local bucket=$1
  if [[ ! "$bucket" ]]; then
    echo -e "${RED}no arg passed to _is_bucket_ext${WHI}"
    return 1
  fi
  aws s3 ls $bucket 1>/dev/null 2>&1
  return $?
}

#######################################
# cp file or dir to folder within bucket
# Globals:
#   None
# Arguments:
#   $1: file or dir to copy to S3
#   $2 = bucket/ext to copy to
#######################################
function _put() {

  ### Local params
  local source=$1
  local target=$2
  local result_code=1

  ### Check args aren't null
  if [[ -z $source || -z $target ]]; then
    echo "_put must have 2 args"
    return 1
  fi

  ### Make sure $source is a file or dir
  if [[ ! -f "$source" ]] && [[ ! -d "$source" ]]; then
    echo -e "${RED} $source is not a file or a dir! ${WHI}"
    return 1
  fi

  ### Throw error if target ends in '/'
  if [[ $(echo "$target" | sed "s|/$|xxx|g") != $target ]]; then
    echo -e "${RED} Do not terminate your target string with a "/" symbol! "
    return 1
  fi

  ### Throw error if source has a '/' in it
  if [[ $(echo "$source" | sed "s|/|xxx|g") != $source ]]; then
    echo -e "${RED} Your source '$source' cannot have a '/' in it! Only copy a simple file or dir from your PWD!"
    return 1
  fi

  ### Decompose $target into bucket and extens(ions) variables
  local bucket=$(echo "$target" | sed 's@/.*@@')
  local extens=$(echo "$target" | sed 's@[^/]*@@')

  ### Check bucket exists
  if ! _does_bucket_exist $bucket; then
    echo -e "${RED}Bucket/ext '$bucket' does not exist for these credentials${WHI}"
    return 1
  fi

  ### If $source is a file
  if [[ -f $source ]]; then
    ### Try uploading
    cmd="aws s3 cp $source s3://$target/"
    echo -e "${BLU}Running: ${cmd}${WHI}"
    bash -c "$cmd" 1>/dev/null
    result_code=$?
    ### Print result
    if [[ result_code -eq 0 ]]; then
      local new_route="https://$bucket.s3.amazonaws.com${extens}${source}"
      echo -e """${GRE}
      Success! Try: ${CYA}${new_route}${extens}${source}
      ${WHI}"""
    else
      echo -e "${RED} Hmmmm... sth went wrong uploading file ${source}!${WHI}"
      return 1
    fi
  fi

  echo -e "${RED}====================="
  echo -e "target: $target"
  echo -e "bucket: $bucket"
  echo -e "source: $source"
  echo -e "extens: $extens"
  echo -e "=====================${WHI}"

  ### If $source is a dir
  if [[ -d $source ]]; then
    ### Basic recursive upload
    echo -e "${MAG}>>> STEP 1: Basic-recursive upload:${WHI}"
    cmd="aws s3 cp --recursive $source s3://${bucket}${extens}"
    echo -e "${BLU}>>> ${cmd}${WHI}"
    bash -c "$cmd" 1>/dev/null
    result_code=$?
    if [[ $result_code -ne 0 ]]; then
      echo -e "${RED}Initial upload unsuccessful${WHI}"
      return 1
    fi

    ### Create copy of each html file without extension and upload
    echo -e "${MAG}>>> STEP 2: HTML-sans-extension copy:${WHI}"
    declare -a html_files=($(find $source -type f -name "*.html"))
    for html_file in "${html_files[@]}"; do
      local html_file_sans_ext="${html_file%.*}"
      local relative_path_to_file=${html_file_sans_ext#./}
      cp $html_file $html_file_sans_ext
      cmd="aws s3 cp --content-type \"text/html\" $html_file_sans_ext s3://${target}/${relative_path_to_file}"
      echo -e "${BLU}>>> ${cmd}${WHI}"
      bash -c "$cmd" 1>/dev/null
      result_code=$?
      if [[ $result_code -ne 0 ]]; then
        echo -e "${RED}HTML-twin upload unsuccessful${WHI}"
        return 1
      fi
      rm $html_file_sans_ext
    done

    ### Find each image file and pass its path and mime type as args to_zip_and_upload_image_file
    ### NOTE: you do NOT want to gzip mp4 files!
    echo -e "${MAG}>>> STEP 3: Zipped-media copy:${WHI}"
    declare -a media_files=(
      $(
        find $source -type f |
          xargs file --mime-type |
          grep -E "image\/" |
          tr -s ' ' |
          sed 's/://g' |
          cut -d' ' -f1
      )
    )
    for media_file in "${media_files[@]}"; do
      local mime_type=$(file --mime-type $media_file | cut -d' ' -f2)
      ### zip file
      gzip -kf9 $media_file
      local path_to_zipped_file="$media_file.gz"
      ### Upload zipped file to S3
      local relative_path_to_file=${media_file##.}
      cmd="aws s3 cp --content-type \"${mime_type}\" --content-encoding \"gzip\" ${path_to_zipped_file} s3://${target}${relative_path_to_file}"
      echo -e "${BLU}>>> ${cmd}${WHI}"
      bash -c "$cmd" 1>/dev/null
      result_code=$?
      if [[ $result_code -ne 0 ]]; then
        echo -e "${RED}Zipped-file upload unsuccessful${WHI}"
        return 1
      fi
      rm $path_to_zipped_file
    done

    ### Print final message
    local new_route="https://$bucket.s3.amazonaws.com${extens}"
    echo -e "${GRE}Success! Try:${CYA} ${new_route} ${WHI}"
  fi

}

main() {
  ### Handle no args
  if [[ $# -eq 0 ]]; then
    _usage
    return 1
  fi

  ### Get 1st arg and shift
  option=$1
  shift

  ### Branch logic
  case $option in
  help | --help)
    _usage
    ;;
  ls)
    _ls
    ;;
  put)
    ### Get 2nd arg and shift
    if [[ ! "$1" || ! "$2" ]]; then
      echo -e "${RED}put requires two args: FILE/DIR BUCKET[/ext]${WHI}"
      return 1
    fi
    _put $1 $2
    ;;
  *)
    echo >&2 "Invalid option: \"${option}\""
    _usage
    return 1
    ;;
  esac
}

### Execute main with all arguments passed to script
main "$@"
